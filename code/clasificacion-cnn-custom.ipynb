{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.applications import ResNet50, VGG16, VGG19, InceptionResNetV2\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_height = 150\n",
    "img_width = 150\n",
    "batch_size = 32\n",
    "data_directory = '/Users/baudi/AI/practicas/uvas/data/train_val/'\n",
    "test_data_directory = '/Users/baudi/AI/practicas/uvas/data/test/'\n",
    "val_split = 0.2\n",
    "seed = 42\n",
    "num_classes = 4\n",
    "learning_rate = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para ajustar las imágenes de entrenamiento\n",
    "def adjustments(image):\n",
    "    # Ajustar el brillo de la imagen de forma aleatoria entre -0.2 y 0.2\n",
    "    image = tf.image.random_brightness(image, 0.2)\n",
    "    # Ajustar el contraste de la imagen de forma aleatoria entre 0.9 y 1.1\n",
    "    image = tf.image.random_contrast(image, 0.9, 1.1)\n",
    "    return image\n",
    "\n",
    "# Función para procesar las etiquetas\n",
    "def process_labels(images, labels):\n",
    "    # Convertir las etiquetas a one-hot encoding\n",
    "    one_hot_labels = tf.one_hot(labels, depth=num_classes)\n",
    "    return images, one_hot_labels\n",
    "\n",
    "# Cargar los datos de entrenamiento\n",
    "train_data_raw = image_dataset_from_directory(\n",
    "    data_directory,                     # Directorio donde se encuentran las imágenes\n",
    "    validation_split=val_split,         # Porcentaje del dataset a usar como validación\n",
    "    subset=\"training\",                  # Subconjunto de datos a utilizar\n",
    "    seed=seed,                          # Semilla aleatoria para la reproducibilidad\n",
    "    image_size=(img_height, img_width), # Tamaño de las imágenes\n",
    "    batch_size=batch_size               # Tamaño del batch de entrenamiento\n",
    ")\n",
    "\n",
    "# Obtener los nombres de las clases\n",
    "class_names = train_data_raw.class_names\n",
    "\n",
    "# Aplicar las funciones de ajuste y procesamiento de etiquetas a los datos de entrenamiento\n",
    "train_data = train_data_raw.map(lambda x, y: (adjustments(x), y)).map(process_labels).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Cargar los datos de validación\n",
    "val_data = image_dataset_from_directory(\n",
    "    data_directory,                     # Directorio donde se encuentran las imágenes\n",
    "    validation_split=val_split,         # Porcentaje del dataset a usar como validación\n",
    "    subset=\"validation\",                # Subconjunto de datos a utilizar\n",
    "    seed=seed,                          # Semilla aleatoria para la reproducibilidad\n",
    "    image_size=(img_height, img_width), # Tamaño de las imágenes\n",
    "    batch_size=batch_size               # Tamaño del batch de validación\n",
    ").map(process_labels).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Cargar los datos de prueba\n",
    "test_data = image_dataset_from_directory(\n",
    "    test_data_directory,                # Directorio donde se encuentran las imágenes de prueba\n",
    "    seed=seed,                          # Semilla aleatoria para la reproducibilidad\n",
    "    image_size=(img_height, img_width), # Tamaño de las imágenes de prueba\n",
    "    batch_size=batch_size               # Tamaño del batch de prueba\n",
    ").map(process_labels).cache().prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    # Cargar la red pre-entrenada VGG19\n",
    "    base_model = VGG19(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "    \n",
    "    # Congelar todas las capas de la red base excepto las últimas 12\n",
    "    for layer in base_model.layers[:-12]:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # Añadir capas adicionales para la clasificación\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    # Crear el modelo final\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    \n",
    "    # Compilar el modelo\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(model_builder, n_splits=2, epochs=20):\n",
    "    # Crear un objeto KFold para dividir los datos en los folds de entrenamiento y validación\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # Concatenar las imágenes y etiquetas de entrenamiento y validación en dos arrays separados\n",
    "    X = np.concatenate([x for x, y in train_data] + [x for x, y in val_data])\n",
    "    y = np.concatenate([y for x, y in train_data] + [y for x, y in val_data])\n",
    "\n",
    "    # Lista para guardar las precisiones de validación de cada fold\n",
    "    accuracies = []\n",
    "    # Iterar sobre cada fold\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y), start=1):\n",
    "        # Construir el modelo\n",
    "        model = model_builder()\n",
    "        print(f\"Comienza el entrenamiento del fold {fold}\")\n",
    "        # Definir los callbacks de EarlyStopping y ReduceLROnPlateau\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=0.00001)\n",
    "        callbacks = [early_stopping, reduce_lr]\n",
    "\n",
    "        # Entrenar el modelo en los datos del fold actual\n",
    "        history = model.fit(X[train_idx],\n",
    "                  y[train_idx],\n",
    "                  validation_data=(X[val_idx], y[val_idx]),\n",
    "                  batch_size = batch_size,\n",
    "                  epochs=epochs,\n",
    "                  callbacks=callbacks)\n",
    "        print(f\"Termina el entrenamiento del fold {fold}\")\n",
    "        # Guardar la precisión de validación del último epoch del entrenamiento del modelo\n",
    "        val_accuracy = history.history['val_accuracy']\n",
    "        last_val_accuracy = val_accuracy[-1]\n",
    "        print(f\"Precisión de validación del fold {fold}: {last_val_accuracy}\")\n",
    "\n",
    "        accuracies.append(last_val_accuracy)\n",
    "\n",
    "    # Calcular y devolver la media de las precisiones de validación de todos los folds\n",
    "    return np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realizar la validación cruzada\n",
    "mean_accuracy = cross_validate(build_model,5,50)\n",
    "print(f'Mean accuracy: {mean_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_final_model(model_builder, epochs=20):\n",
    "    # Crear el modelo final utilizando el modelo_builder\n",
    "    final_model = model_builder()\n",
    "\n",
    "    # Concatenar los datos de entrenamiento y validación\n",
    "    X = np.concatenate([x for x, y in train_data])\n",
    "    y = np.concatenate([y for x, y in train_data])\n",
    "\n",
    "    # Definir el callback de EarlyStopping y ReduceLROnPlateau\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=0.00001)\n",
    "\n",
    "    # Agregar los callbacks al método fit\n",
    "    callbacks = [early_stopping, reduce_lr]\n",
    "    final_model.fit(\n",
    "        X,\n",
    "        y, \n",
    "        epochs=epochs, \n",
    "        batch_size=batch_size, \n",
    "        validation_data=val_data, \n",
    "        callbacks=callbacks)\n",
    "\n",
    "    # Devolver el modelo final entrenado\n",
    "    return final_model\n",
    "\n",
    "# Entrenar el modelo final utilizando build_model y 50 epochs\n",
    "final_model = train_final_model(build_model,50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el modelo en el conjunto de prueba\n",
    "test_loss, test_accuracy = final_model.evaluate(test_data)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Predecir el conjunto de prueba\n",
    "Y_pred = final_model.predict(test_data)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "# Obtener las etiquetas del conjunto de prueba\n",
    "y_true = np.concatenate([y.numpy() for _, y in test_data.unbatch()])\n",
    "y_true_labels = np.argmax(y_true.reshape(-1, len(class_names)), axis=1)\n",
    "\n",
    "# Mostrar el informe de clasificación\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_true_labels, y_pred, target_names=class_names))\n",
    "\n",
    "# Mostrar la matriz de confusión\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_true_labels, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
