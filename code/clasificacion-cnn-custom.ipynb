{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.applications import ResNet50, VGG16, VGG19, InceptionResNetV2\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_height = 150\n",
    "img_width = 150\n",
    "batch_size = 32\n",
    "data_directory = '/Users/baudi/AI/practicas/uvas/data/train_val/'\n",
    "test_data_directory = '/Users/baudi/AI/practicas/uvas/data/test/'\n",
    "val_split = 0.2\n",
    "seed = 42\n",
    "num_classes = 4\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3248 files belonging to 4 classes.\n",
      "Using 2599 files for training.\n",
      "Metal device set to: Apple M2 Pro\n",
      "\n",
      "systemMemory: 32.00 GB\n",
      "maxCacheSize: 10.67 GB\n",
      "\n",
      "Found 3248 files belonging to 4 classes.\n",
      "Using 649 files for validation.\n",
      "Found 814 files belonging to 4 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-10 22:13:48.152207: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-04-10 22:13:48.152362: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Función para ajustar las imágenes de entrenamiento\n",
    "def adjustments(image):\n",
    "    # Ajustar el brillo de la imagen de forma aleatoria entre -0.2 y 0.2\n",
    "    image = tf.image.random_brightness(image, 0.2)\n",
    "    # Ajustar el contraste de la imagen de forma aleatoria entre 0.9 y 1.1\n",
    "    image = tf.image.random_contrast(image, 0.9, 1.1)\n",
    "    return image\n",
    "\n",
    "# Función para procesar las etiquetas\n",
    "def process_labels(images, labels):\n",
    "    # Convertir las etiquetas a one-hot encoding\n",
    "    one_hot_labels = tf.one_hot(labels, depth=num_classes)\n",
    "    return images, one_hot_labels\n",
    "\n",
    "# Cargar los datos de entrenamiento\n",
    "train_data_raw = image_dataset_from_directory(\n",
    "    data_directory,                     # Directorio donde se encuentran las imágenes\n",
    "    validation_split=val_split,         # Porcentaje del dataset a usar como validación\n",
    "    subset=\"training\",                  # Subconjunto de datos a utilizar\n",
    "    seed=seed,                          # Semilla aleatoria para la reproducibilidad\n",
    "    image_size=(img_height, img_width), # Tamaño de las imágenes\n",
    "    batch_size=batch_size               # Tamaño del batch de entrenamiento\n",
    ")\n",
    "\n",
    "# Obtener los nombres de las clases\n",
    "class_names = train_data_raw.class_names\n",
    "\n",
    "# Aplicar las funciones de ajuste y procesamiento de etiquetas a los datos de entrenamiento\n",
    "train_data = train_data_raw.map(lambda x, y: (adjustments(x), y)).map(process_labels).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Cargar los datos de validación\n",
    "val_data = image_dataset_from_directory(\n",
    "    data_directory,                     # Directorio donde se encuentran las imágenes\n",
    "    validation_split=val_split,         # Porcentaje del dataset a usar como validación\n",
    "    subset=\"validation\",                # Subconjunto de datos a utilizar\n",
    "    seed=seed,                          # Semilla aleatoria para la reproducibilidad\n",
    "    image_size=(img_height, img_width), # Tamaño de las imágenes\n",
    "    batch_size=batch_size               # Tamaño del batch de validación\n",
    ").map(process_labels).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Cargar los datos de prueba\n",
    "test_data = image_dataset_from_directory(\n",
    "    test_data_directory,                # Directorio donde se encuentran las imágenes de prueba\n",
    "    seed=seed,                          # Semilla aleatoria para la reproducibilidad\n",
    "    image_size=(img_height, img_width), # Tamaño de las imágenes de prueba\n",
    "    batch_size=batch_size               # Tamaño del batch de prueba\n",
    ").map(process_labels).cache().prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    # Cargar la red pre-entrenada VGG19\n",
    "    base_model = VGG19(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "    \n",
    "    # Congelar todas las capas de la red base excepto las últimas 12\n",
    "    for layer in base_model.layers[:-12]:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # Añadir capas adicionales para la clasificación\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    # Crear el modelo final\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    \n",
    "    # Compilar el modelo\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_custom():\n",
    "\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(150, 150, 3)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        Conv2D(1024, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(2048, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(1024, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(4, activation='softmax')  # 4 clases: sanas, enfermedad1, enfermedad2, enfermedad3\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(model_builder, n_splits=2, epochs=20):\n",
    "    # Crear un objeto KFold para dividir los datos en los folds de entrenamiento y validación\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # Concatenar las imágenes y etiquetas de entrenamiento y validación en dos arrays separados\n",
    "    X = np.concatenate([x for x, y in train_data] + [x for x, y in val_data])\n",
    "    y = np.concatenate([y for x, y in train_data] + [y for x, y in val_data])\n",
    "\n",
    "    # Lista para guardar las precisiones de validación de cada fold\n",
    "    accuracies = []\n",
    "    # Iterar sobre cada fold\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y), start=1):\n",
    "        # Construir el modelo\n",
    "        model = model_builder()\n",
    "        print(f\"Comienza el entrenamiento del fold {fold}\")\n",
    "        # Definir los callbacks de EarlyStopping y ReduceLROnPlateau\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=0.00001)\n",
    "        callbacks = [early_stopping, reduce_lr]\n",
    "\n",
    "        # Entrenar el modelo en los datos del fold actual\n",
    "        history = model.fit(X[train_idx],\n",
    "                  y[train_idx],\n",
    "                  validation_data=(X[val_idx], y[val_idx]),\n",
    "                  batch_size = batch_size,\n",
    "                  epochs=epochs,\n",
    "                  callbacks=callbacks)\n",
    "        print(f\"Termina el entrenamiento del fold {fold}\")\n",
    "        # Guardar la precisión de validación del entrenamiento del modelo\n",
    "        val_accuracy = history.history['val_accuracy']\n",
    "        max_val_accuracy = max(val_accuracy)\n",
    "        print(f\"Precisión de validación del fold {fold}: {max_val_accuracy}\")\n",
    "\n",
    "        accuracies.append(max_val_accuracy)\n",
    "\n",
    "    # Calcular y devolver la media de las precisiones de validación de todos los folds\n",
    "    return np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-10 22:13:48.405819: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comienza el entrenamiento del fold 1\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-10 22:13:49.870494: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - ETA: 0s - loss: 1.0897 - accuracy: 0.6501"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-10 22:13:55.623110: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - 7s 70ms/step - loss: 1.0897 - accuracy: 0.6501 - val_loss: 1.5333 - val_accuracy: 0.3585 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "82/82 [==============================] - 5s 62ms/step - loss: 0.6530 - accuracy: 0.7841 - val_loss: 0.7949 - val_accuracy: 0.6277 - lr: 1.0000e-04\n",
      "Epoch 3/50\n",
      "82/82 [==============================] - 5s 62ms/step - loss: 0.4650 - accuracy: 0.8414 - val_loss: 0.4839 - val_accuracy: 0.7985 - lr: 1.0000e-04\n",
      "Epoch 4/50\n",
      "82/82 [==============================] - 5s 62ms/step - loss: 0.3946 - accuracy: 0.8649 - val_loss: 0.2939 - val_accuracy: 0.8908 - lr: 1.0000e-04\n",
      "Epoch 5/50\n",
      "82/82 [==============================] - 5s 62ms/step - loss: 0.3309 - accuracy: 0.8853 - val_loss: 0.2275 - val_accuracy: 0.9138 - lr: 1.0000e-04\n",
      "Epoch 6/50\n",
      "82/82 [==============================] - 5s 61ms/step - loss: 0.2367 - accuracy: 0.9169 - val_loss: 0.2562 - val_accuracy: 0.8969 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "82/82 [==============================] - 5s 62ms/step - loss: 0.2080 - accuracy: 0.9273 - val_loss: 0.1891 - val_accuracy: 0.9277 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "82/82 [==============================] - 5s 62ms/step - loss: 0.1676 - accuracy: 0.9457 - val_loss: 0.1492 - val_accuracy: 0.9492 - lr: 1.0000e-04\n",
      "Epoch 9/50\n",
      "82/82 [==============================] - 5s 62ms/step - loss: 0.1184 - accuracy: 0.9565 - val_loss: 0.1426 - val_accuracy: 0.9415 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "82/82 [==============================] - 5s 63ms/step - loss: 0.1319 - accuracy: 0.9554 - val_loss: 0.1408 - val_accuracy: 0.9508 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "82/82 [==============================] - 5s 62ms/step - loss: 0.1278 - accuracy: 0.9600 - val_loss: 0.1391 - val_accuracy: 0.9600 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "82/82 [==============================] - 5s 62ms/step - loss: 0.0904 - accuracy: 0.9684 - val_loss: 0.1195 - val_accuracy: 0.9538 - lr: 1.0000e-04\n",
      "Epoch 13/50\n",
      "82/82 [==============================] - 5s 62ms/step - loss: 0.0763 - accuracy: 0.9719 - val_loss: 0.2257 - val_accuracy: 0.9262 - lr: 1.0000e-04\n",
      "Epoch 14/50\n",
      "82/82 [==============================] - 5s 62ms/step - loss: 0.0492 - accuracy: 0.9838 - val_loss: 0.1535 - val_accuracy: 0.9492 - lr: 1.0000e-04\n",
      "Epoch 15/50\n",
      "82/82 [==============================] - 5s 62ms/step - loss: 0.0956 - accuracy: 0.9696 - val_loss: 0.1384 - val_accuracy: 0.9538 - lr: 1.0000e-04\n",
      "Epoch 16/50\n",
      "82/82 [==============================] - 5s 66ms/step - loss: 0.0843 - accuracy: 0.9711 - val_loss: 0.1122 - val_accuracy: 0.9662 - lr: 1.0000e-05\n",
      "Epoch 17/50\n",
      "82/82 [==============================] - 5s 62ms/step - loss: 0.0562 - accuracy: 0.9788 - val_loss: 0.1042 - val_accuracy: 0.9677 - lr: 1.0000e-05\n",
      "Epoch 18/50\n",
      "82/82 [==============================] - 5s 61ms/step - loss: 0.0599 - accuracy: 0.9788 - val_loss: 0.1028 - val_accuracy: 0.9692 - lr: 1.0000e-05\n",
      "Epoch 19/50\n",
      "82/82 [==============================] - 5s 62ms/step - loss: 0.0360 - accuracy: 0.9869 - val_loss: 0.0961 - val_accuracy: 0.9723 - lr: 1.0000e-05\n",
      "Epoch 20/50\n",
      "82/82 [==============================] - 5s 63ms/step - loss: 0.0407 - accuracy: 0.9885 - val_loss: 0.0953 - val_accuracy: 0.9738 - lr: 1.0000e-05\n",
      "Epoch 21/50\n",
      "82/82 [==============================] - 5s 65ms/step - loss: 0.0369 - accuracy: 0.9885 - val_loss: 0.0903 - val_accuracy: 0.9738 - lr: 1.0000e-05\n",
      "Epoch 22/50\n",
      "82/82 [==============================] - 5s 63ms/step - loss: 0.0388 - accuracy: 0.9911 - val_loss: 0.0888 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 23/50\n",
      "82/82 [==============================] - 5s 62ms/step - loss: 0.0354 - accuracy: 0.9869 - val_loss: 0.0893 - val_accuracy: 0.9723 - lr: 1.0000e-05\n",
      "Epoch 24/50\n",
      "82/82 [==============================] - 5s 63ms/step - loss: 0.0318 - accuracy: 0.9904 - val_loss: 0.0886 - val_accuracy: 0.9738 - lr: 1.0000e-05\n",
      "Epoch 25/50\n",
      "82/82 [==============================] - 5s 62ms/step - loss: 0.0279 - accuracy: 0.9931 - val_loss: 0.0930 - val_accuracy: 0.9723 - lr: 1.0000e-05\n",
      "Epoch 26/50\n",
      "82/82 [==============================] - 5s 62ms/step - loss: 0.0300 - accuracy: 0.9908 - val_loss: 0.0939 - val_accuracy: 0.9692 - lr: 1.0000e-05\n",
      "Epoch 27/50\n",
      "82/82 [==============================] - 5s 62ms/step - loss: 0.0234 - accuracy: 0.9927 - val_loss: 0.0936 - val_accuracy: 0.9738 - lr: 1.0000e-05\n",
      "Epoch 28/50\n",
      "82/82 [==============================] - 5s 63ms/step - loss: 0.0270 - accuracy: 0.9911 - val_loss: 0.0884 - val_accuracy: 0.9738 - lr: 1.0000e-05\n",
      "Epoch 29/50\n",
      "82/82 [==============================] - 5s 62ms/step - loss: 0.0364 - accuracy: 0.9908 - val_loss: 0.0895 - val_accuracy: 0.9754 - lr: 1.0000e-05\n",
      "Epoch 30/50\n",
      "82/82 [==============================] - 5s 63ms/step - loss: 0.0232 - accuracy: 0.9915 - val_loss: 0.0851 - val_accuracy: 0.9785 - lr: 1.0000e-05\n",
      "Epoch 31/50\n",
      "82/82 [==============================] - 5s 63ms/step - loss: 0.0194 - accuracy: 0.9927 - val_loss: 0.0848 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 32/50\n",
      "82/82 [==============================] - 5s 63ms/step - loss: 0.0196 - accuracy: 0.9935 - val_loss: 0.0842 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 33/50\n",
      "82/82 [==============================] - 5s 63ms/step - loss: 0.0158 - accuracy: 0.9946 - val_loss: 0.0827 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 34/50\n",
      "82/82 [==============================] - 5s 63ms/step - loss: 0.0144 - accuracy: 0.9962 - val_loss: 0.0802 - val_accuracy: 0.9785 - lr: 1.0000e-05\n",
      "Epoch 35/50\n",
      "82/82 [==============================] - 5s 62ms/step - loss: 0.0225 - accuracy: 0.9927 - val_loss: 0.0833 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 36/50\n",
      "82/82 [==============================] - 5s 62ms/step - loss: 0.0176 - accuracy: 0.9938 - val_loss: 0.0857 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 37/50\n",
      "82/82 [==============================] - 5s 62ms/step - loss: 0.0131 - accuracy: 0.9969 - val_loss: 0.0851 - val_accuracy: 0.9738 - lr: 1.0000e-05\n",
      "Epoch 38/50\n",
      "82/82 [==============================] - 5s 62ms/step - loss: 0.0204 - accuracy: 0.9938 - val_loss: 0.0826 - val_accuracy: 0.9738 - lr: 1.0000e-05\n",
      "Epoch 39/50\n",
      "82/82 [==============================] - 5s 62ms/step - loss: 0.0136 - accuracy: 0.9962 - val_loss: 0.0823 - val_accuracy: 0.9754 - lr: 1.0000e-05\n",
      "Epoch 40/50\n",
      "82/82 [==============================] - 5s 62ms/step - loss: 0.0155 - accuracy: 0.9950 - val_loss: 0.0836 - val_accuracy: 0.9738 - lr: 1.0000e-05\n",
      "Epoch 41/50\n",
      "82/82 [==============================] - 5s 62ms/step - loss: 0.0112 - accuracy: 0.9958 - val_loss: 0.0835 - val_accuracy: 0.9738 - lr: 1.0000e-05\n",
      "Epoch 42/50\n",
      "82/82 [==============================] - 5s 63ms/step - loss: 0.0153 - accuracy: 0.9950 - val_loss: 0.0802 - val_accuracy: 0.9785 - lr: 1.0000e-05\n",
      "Epoch 43/50\n",
      "82/82 [==============================] - 5s 64ms/step - loss: 0.0108 - accuracy: 0.9969 - val_loss: 0.0837 - val_accuracy: 0.9785 - lr: 1.0000e-05\n",
      "Epoch 44/50\n",
      "82/82 [==============================] - 5s 64ms/step - loss: 0.0125 - accuracy: 0.9969 - val_loss: 0.0795 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 45/50\n",
      "82/82 [==============================] - 5s 63ms/step - loss: 0.0123 - accuracy: 0.9973 - val_loss: 0.0820 - val_accuracy: 0.9785 - lr: 1.0000e-05\n",
      "Epoch 46/50\n",
      "82/82 [==============================] - 5s 63ms/step - loss: 0.0109 - accuracy: 0.9965 - val_loss: 0.0801 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 47/50\n",
      "82/82 [==============================] - 5s 62ms/step - loss: 0.0145 - accuracy: 0.9954 - val_loss: 0.0833 - val_accuracy: 0.9738 - lr: 1.0000e-05\n",
      "Epoch 48/50\n",
      "82/82 [==============================] - 5s 63ms/step - loss: 0.0130 - accuracy: 0.9969 - val_loss: 0.0732 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 49/50\n",
      "82/82 [==============================] - 5s 62ms/step - loss: 0.0084 - accuracy: 0.9981 - val_loss: 0.0739 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 50/50\n",
      "82/82 [==============================] - 5s 63ms/step - loss: 0.0112 - accuracy: 0.9969 - val_loss: 0.0743 - val_accuracy: 0.9800 - lr: 1.0000e-05\n",
      "Termina el entrenamiento del fold 1\n",
      "Precisión de validación del fold 1: 0.9800000190734863\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'last_val_accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#Realizar la validación cruzada\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m mean_accuracy \u001b[39m=\u001b[39m cross_validate(build_model_custom,\u001b[39m5\u001b[39;49m,\u001b[39m50\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mMean accuracy: \u001b[39m\u001b[39m{\u001b[39;00mmean_accuracy\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 33\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(model_builder, n_splits, epochs)\u001b[0m\n\u001b[1;32m     30\u001b[0m     max_val_accuracy \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(val_accuracy)\n\u001b[1;32m     31\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPrecisión de validación del fold \u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m{\u001b[39;00mmax_val_accuracy\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m     accuracies\u001b[39m.\u001b[39mappend(last_val_accuracy)\n\u001b[1;32m     35\u001b[0m \u001b[39m# Calcular y devolver la media de las precisiones de validación de todos los folds\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(accuracies)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'last_val_accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "#Realizar la validación cruzada\n",
    "mean_accuracy = cross_validate(build_model_custom,5,50)\n",
    "print(f'Mean accuracy: {mean_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_final_model(model_builder, epochs=20):\n",
    "    # Crear el modelo final utilizando el modelo_builder\n",
    "    final_model = model_builder()\n",
    "\n",
    "    # Concatenar los datos de entrenamiento y validación\n",
    "    X = np.concatenate([x for x, y in train_data])\n",
    "    y = np.concatenate([y for x, y in train_data])\n",
    "\n",
    "    # Definir el callback de EarlyStopping y ReduceLROnPlateau\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=0.00001)\n",
    "\n",
    "    # Agregar los callbacks al método fit\n",
    "    callbacks = [early_stopping, reduce_lr]\n",
    "    final_model.fit(\n",
    "        X,\n",
    "        y, \n",
    "        epochs=epochs, \n",
    "        batch_size=batch_size, \n",
    "        validation_data=val_data, \n",
    "        callbacks=callbacks)\n",
    "\n",
    "    # Devolver el modelo final entrenado\n",
    "    return final_model\n",
    "\n",
    "# Entrenar el modelo final utilizando build_model y 50 epochs\n",
    "final_model = train_final_model(build_model_custom,50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el modelo en el conjunto de prueba\n",
    "test_loss, test_accuracy = final_model.evaluate(test_data)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Predecir el conjunto de prueba\n",
    "Y_pred = final_model.predict(test_data)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "# Obtener las etiquetas del conjunto de prueba\n",
    "y_true = np.concatenate([y.numpy() for _, y in test_data.unbatch()])\n",
    "y_true_labels = np.argmax(y_true.reshape(-1, len(class_names)), axis=1)\n",
    "\n",
    "# Mostrar el informe de clasificación\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_true_labels, y_pred, target_names=class_names))\n",
    "\n",
    "# Mostrar la matriz de confusión\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_true_labels, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
