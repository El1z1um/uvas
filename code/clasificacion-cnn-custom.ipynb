{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.applications import ResNet50, VGG16, VGG19, InceptionResNetV2\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img_height = 150\n",
    "img_width = 150\n",
    "batch_size = 32\n",
    "data_directory = '/Users/baudi/AI/practicas/uvas/data/train_val/'\n",
    "test_data_directory = '/Users/baudi/AI/practicas/uvas/data/test/'\n",
    "val_split = 0.2\n",
    "seed = 42\n",
    "num_classes = 4\n",
    "learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3248 files belonging to 4 classes.\n",
      "Using 2599 files for training.\n",
      "Metal device set to: Apple M2 Pro\n",
      "\n",
      "systemMemory: 32.00 GB\n",
      "maxCacheSize: 10.67 GB\n",
      "\n",
      "Found 3248 files belonging to 4 classes.\n",
      "Using 649 files for validation.\n",
      "Found 814 files belonging to 4 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-10 22:13:48.152207: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-04-10 22:13:48.152362: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "# Función para ajustar las imágenes de entrenamiento\n",
    "def adjustments(image):\n",
    "    # Ajustar el brillo de la imagen de forma aleatoria entre -0.2 y 0.2\n",
    "    image = tf.image.random_brightness(image, 0.2)\n",
    "    # Ajustar el contraste de la imagen de forma aleatoria entre 0.9 y 1.1\n",
    "    image = tf.image.random_contrast(image, 0.9, 1.1)\n",
    "    return image\n",
    "\n",
    "# Función para procesar las etiquetas\n",
    "def process_labels(images, labels):\n",
    "    # Convertir las etiquetas a one-hot encoding\n",
    "    one_hot_labels = tf.one_hot(labels, depth=num_classes)\n",
    "    return images, one_hot_labels\n",
    "\n",
    "# Cargar los datos de entrenamiento\n",
    "train_data_raw = image_dataset_from_directory(\n",
    "    data_directory,                     # Directorio donde se encuentran las imágenes\n",
    "    validation_split=val_split,         # Porcentaje del dataset a usar como validación\n",
    "    subset=\"training\",                  # Subconjunto de datos a utilizar\n",
    "    seed=seed,                          # Semilla aleatoria para la reproducibilidad\n",
    "    image_size=(img_height, img_width), # Tamaño de las imágenes\n",
    "    batch_size=batch_size               # Tamaño del batch de entrenamiento\n",
    ")\n",
    "\n",
    "# Obtener los nombres de las clases\n",
    "class_names = train_data_raw.class_names\n",
    "\n",
    "# Aplicar las funciones de ajuste y procesamiento de etiquetas a los datos de entrenamiento\n",
    "train_data = train_data_raw.map(lambda x, y: (adjustments(x), y)).map(process_labels).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Cargar los datos de validación\n",
    "val_data = image_dataset_from_directory(\n",
    "    data_directory,                     # Directorio donde se encuentran las imágenes\n",
    "    validation_split=val_split,         # Porcentaje del dataset a usar como validación\n",
    "    subset=\"validation\",                # Subconjunto de datos a utilizar\n",
    "    seed=seed,                          # Semilla aleatoria para la reproducibilidad\n",
    "    image_size=(img_height, img_width), # Tamaño de las imágenes\n",
    "    batch_size=batch_size               # Tamaño del batch de validación\n",
    ").map(process_labels).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Cargar los datos de prueba\n",
    "test_data = image_dataset_from_directory(\n",
    "    test_data_directory,                # Directorio donde se encuentran las imágenes de prueba\n",
    "    seed=seed,                          # Semilla aleatoria para la reproducibilidad\n",
    "    image_size=(img_height, img_width), # Tamaño de las imágenes de prueba\n",
    "    batch_size=batch_size               # Tamaño del batch de prueba\n",
    ").map(process_labels).cache().prefetch(tf.data.experimental.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    # Cargar la red pre-entrenada VGG19\n",
    "    base_model = VGG19(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "    \n",
    "    # Congelar todas las capas de la red base excepto las últimas 12\n",
    "    for layer in base_model.layers[:-12]:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # Añadir capas adicionales para la clasificación\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    # Crear el modelo final\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    \n",
    "    # Compilar el modelo\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_custom():\n",
    "\n",
    "    model = Sequential([\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(150, 150, 3)),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        \n",
    "        Conv2D(256, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        Conv2D(512, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        Conv2D(1024, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        Conv2D(2048, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        Conv2D(4096, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(2048, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(4, activation='softmax')  # 4 clases: sanas, enfermedad1, enfermedad2, enfermedad3\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(model_builder, n_splits=2, epochs=20):\n",
    "    # Crear un objeto KFold para dividir los datos en los folds de entrenamiento y validación\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    # Concatenar las imágenes y etiquetas de entrenamiento y validación en dos arrays separados\n",
    "    X = np.concatenate([x for x, y in train_data] + [x for x, y in val_data])\n",
    "    y = np.concatenate([y for x, y in train_data] + [y for x, y in val_data])\n",
    "\n",
    "    # Lista para guardar las precisiones de validación de cada fold\n",
    "    accuracies = []\n",
    "    # Iterar sobre cada fold\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y), start=1):\n",
    "        # Construir el modelo\n",
    "        model = model_builder()\n",
    "        print(f\"Comienza el entrenamiento del fold {fold}\")\n",
    "        # Definir los callbacks de EarlyStopping y ReduceLROnPlateau\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=0.00001)\n",
    "        callbacks = [early_stopping, reduce_lr]\n",
    "\n",
    "        # Entrenar el modelo en los datos del fold actual\n",
    "        history = model.fit(X[train_idx],\n",
    "                  y[train_idx],\n",
    "                  validation_data=(X[val_idx], y[val_idx]),\n",
    "                  batch_size = batch_size,\n",
    "                  epochs=epochs,\n",
    "                  callbacks=callbacks)\n",
    "        print(f\"Termina el entrenamiento del fold {fold}\")\n",
    "        # Guardar la precisión de validación del entrenamiento del modelo\n",
    "        val_accuracy = history.history['val_accuracy']\n",
    "        max_val_accuracy = max(val_accuracy)\n",
    "        print(f\"Precisión de validación del fold {fold}: {max_val_accuracy}\")\n",
    "\n",
    "        accuracies.append(max_val_accuracy)\n",
    "\n",
    "    # Calcular y devolver la media de las precisiones de validación de todos los folds\n",
    "    return np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comienza el entrenamiento del fold 1\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-10 22:37:41.654627: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - ETA: 0s - loss: 0.7850 - accuracy: 0.7937"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-10 22:37:58.672374: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - 19s 202ms/step - loss: 0.7850 - accuracy: 0.7937 - val_loss: 1.0275 - val_accuracy: 0.5308 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "82/82 [==============================] - 15s 184ms/step - loss: 0.2604 - accuracy: 0.9119 - val_loss: 1.1879 - val_accuracy: 0.6923 - lr: 1.0000e-04\n",
      "Epoch 3/50\n",
      "82/82 [==============================] - 15s 185ms/step - loss: 0.2080 - accuracy: 0.9303 - val_loss: 0.7990 - val_accuracy: 0.7600 - lr: 1.0000e-04\n",
      "Epoch 4/50\n",
      "82/82 [==============================] - 15s 185ms/step - loss: 0.1420 - accuracy: 0.9607 - val_loss: 0.4723 - val_accuracy: 0.8538 - lr: 1.0000e-04\n",
      "Epoch 5/50\n",
      "82/82 [==============================] - 15s 181ms/step - loss: 0.0982 - accuracy: 0.9692 - val_loss: 0.6812 - val_accuracy: 0.8338 - lr: 1.0000e-04\n",
      "Epoch 6/50\n",
      "82/82 [==============================] - 15s 183ms/step - loss: 0.1042 - accuracy: 0.9665 - val_loss: 0.2814 - val_accuracy: 0.9185 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "82/82 [==============================] - 15s 183ms/step - loss: 0.0669 - accuracy: 0.9777 - val_loss: 0.3363 - val_accuracy: 0.9354 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "82/82 [==============================] - 15s 186ms/step - loss: 0.1178 - accuracy: 0.9673 - val_loss: 0.3207 - val_accuracy: 0.9277 - lr: 1.0000e-04\n",
      "Epoch 9/50\n",
      "82/82 [==============================] - 15s 184ms/step - loss: 0.0998 - accuracy: 0.9711 - val_loss: 0.2171 - val_accuracy: 0.9492 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "82/82 [==============================] - 15s 184ms/step - loss: 0.0896 - accuracy: 0.9669 - val_loss: 0.1784 - val_accuracy: 0.9492 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "82/82 [==============================] - 15s 181ms/step - loss: 0.0663 - accuracy: 0.9769 - val_loss: 0.5872 - val_accuracy: 0.8554 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "82/82 [==============================] - 15s 183ms/step - loss: 0.0443 - accuracy: 0.9896 - val_loss: 0.0992 - val_accuracy: 0.9738 - lr: 1.0000e-04\n",
      "Epoch 13/50\n",
      "82/82 [==============================] - 15s 181ms/step - loss: 0.0568 - accuracy: 0.9838 - val_loss: 0.7238 - val_accuracy: 0.8538 - lr: 1.0000e-04\n",
      "Epoch 14/50\n",
      "82/82 [==============================] - 15s 181ms/step - loss: 0.0273 - accuracy: 0.9900 - val_loss: 0.2059 - val_accuracy: 0.9446 - lr: 1.0000e-04\n",
      "Epoch 15/50\n",
      "82/82 [==============================] - 15s 182ms/step - loss: 0.0575 - accuracy: 0.9815 - val_loss: 0.2230 - val_accuracy: 0.9492 - lr: 1.0000e-04\n",
      "Epoch 16/50\n",
      "82/82 [==============================] - 15s 184ms/step - loss: 0.0324 - accuracy: 0.9892 - val_loss: 0.0773 - val_accuracy: 0.9723 - lr: 1.0000e-05\n",
      "Epoch 17/50\n",
      "82/82 [==============================] - 15s 185ms/step - loss: 0.0137 - accuracy: 0.9965 - val_loss: 0.0621 - val_accuracy: 0.9785 - lr: 1.0000e-05\n",
      "Epoch 18/50\n",
      "82/82 [==============================] - 15s 183ms/step - loss: 0.0104 - accuracy: 0.9965 - val_loss: 0.0534 - val_accuracy: 0.9831 - lr: 1.0000e-05\n",
      "Epoch 19/50\n",
      "82/82 [==============================] - 15s 181ms/step - loss: 0.0101 - accuracy: 0.9969 - val_loss: 0.0574 - val_accuracy: 0.9831 - lr: 1.0000e-05\n",
      "Epoch 20/50\n",
      "82/82 [==============================] - 15s 181ms/step - loss: 0.0087 - accuracy: 0.9973 - val_loss: 0.0559 - val_accuracy: 0.9831 - lr: 1.0000e-05\n",
      "Epoch 21/50\n",
      "82/82 [==============================] - 15s 182ms/step - loss: 0.0074 - accuracy: 0.9977 - val_loss: 0.0584 - val_accuracy: 0.9800 - lr: 1.0000e-05\n",
      "Epoch 22/50\n",
      "82/82 [==============================] - 15s 182ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.0460 - val_accuracy: 0.9846 - lr: 1.0000e-05\n",
      "Epoch 23/50\n",
      "82/82 [==============================] - 15s 183ms/step - loss: 0.0039 - accuracy: 0.9985 - val_loss: 0.0463 - val_accuracy: 0.9862 - lr: 1.0000e-05\n",
      "Epoch 24/50\n",
      "82/82 [==============================] - 15s 182ms/step - loss: 0.0048 - accuracy: 0.9988 - val_loss: 0.0493 - val_accuracy: 0.9846 - lr: 1.0000e-05\n",
      "Epoch 25/50\n",
      "82/82 [==============================] - 15s 182ms/step - loss: 0.0049 - accuracy: 0.9992 - val_loss: 0.0575 - val_accuracy: 0.9831 - lr: 1.0000e-05\n",
      "Epoch 26/50\n",
      "82/82 [==============================] - 15s 181ms/step - loss: 0.0032 - accuracy: 0.9996 - val_loss: 0.0513 - val_accuracy: 0.9815 - lr: 1.0000e-05\n",
      "Epoch 27/50\n",
      "82/82 [==============================] - 15s 182ms/step - loss: 0.0026 - accuracy: 0.9992 - val_loss: 0.0522 - val_accuracy: 0.9831 - lr: 1.0000e-05\n",
      "Epoch 28/50\n",
      "82/82 [==============================] - 15s 181ms/step - loss: 0.0100 - accuracy: 0.9962 - val_loss: 0.0603 - val_accuracy: 0.9815 - lr: 1.0000e-05\n",
      "Epoch 29/50\n",
      "82/82 [==============================] - 15s 181ms/step - loss: 0.0095 - accuracy: 0.9977 - val_loss: 0.0685 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 30/50\n",
      "82/82 [==============================] - 15s 181ms/step - loss: 0.0092 - accuracy: 0.9981 - val_loss: 0.0577 - val_accuracy: 0.9785 - lr: 1.0000e-05\n",
      "Epoch 31/50\n",
      "82/82 [==============================] - 15s 181ms/step - loss: 0.0091 - accuracy: 0.9969 - val_loss: 0.0854 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 32/50\n",
      "82/82 [==============================] - 15s 181ms/step - loss: 0.0047 - accuracy: 0.9988 - val_loss: 0.0514 - val_accuracy: 0.9800 - lr: 1.0000e-05\n",
      "Termina el entrenamiento del fold 1\n",
      "Precisión de validación del fold 1: 0.9861538410186768\n",
      "Comienza el entrenamiento del fold 2\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-10 22:45:44.728586: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27/82 [========>.....................] - ETA: 9s - loss: 1.1192 - accuracy: 0.7222 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#Realizar la validación cruzada\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m mean_accuracy \u001b[39m=\u001b[39m cross_validate(build_model_custom,\u001b[39m5\u001b[39;49m,\u001b[39m50\u001b[39;49m)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mMean accuracy: \u001b[39m\u001b[39m{\u001b[39;00mmean_accuracy\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[14], line 21\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(model_builder, n_splits, epochs)\u001b[0m\n\u001b[1;32m     18\u001b[0m callbacks \u001b[39m=\u001b[39m [early_stopping, reduce_lr]\n\u001b[1;32m     20\u001b[0m \u001b[39m# Entrenar el modelo en los datos del fold actual\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X[train_idx],\n\u001b[1;32m     22\u001b[0m           y[train_idx],\n\u001b[1;32m     23\u001b[0m           validation_data\u001b[39m=\u001b[39;49m(X[val_idx], y[val_idx]),\n\u001b[1;32m     24\u001b[0m           batch_size \u001b[39m=\u001b[39;49m batch_size,\n\u001b[1;32m     25\u001b[0m           epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m     26\u001b[0m           callbacks\u001b[39m=\u001b[39;49mcallbacks)\n\u001b[1;32m     27\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTermina el entrenamiento del fold \u001b[39m\u001b[39m{\u001b[39;00mfold\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[39m# Guardar la precisión de validación del entrenamiento del modelo\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1409\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1410\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1411\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2454\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1861\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    498\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    499\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    500\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    501\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    502\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    503\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Realizar la validación cruzada\n",
    "mean_accuracy = cross_validate(build_model_custom,5,50)\n",
    "print(f'Mean accuracy: {mean_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-10 22:46:54.735364: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - ETA: 0s - loss: 1.0886 - accuracy: 0.7176"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-10 22:47:11.114815: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - 19s 219ms/step - loss: 1.0886 - accuracy: 0.7176 - val_loss: 1.4598 - val_accuracy: 0.4807 - lr: 1.0000e-04\n",
      "Epoch 2/50\n",
      "82/82 [==============================] - 16s 194ms/step - loss: 0.7530 - accuracy: 0.8307 - val_loss: 1.2932 - val_accuracy: 0.6271 - lr: 1.0000e-04\n",
      "Epoch 3/50\n",
      "82/82 [==============================] - 16s 191ms/step - loss: 0.4320 - accuracy: 0.8977 - val_loss: 0.4399 - val_accuracy: 0.8505 - lr: 1.0000e-04\n",
      "Epoch 4/50\n",
      "82/82 [==============================] - 16s 193ms/step - loss: 0.3458 - accuracy: 0.9142 - val_loss: 0.3296 - val_accuracy: 0.9214 - lr: 1.0000e-04\n",
      "Epoch 5/50\n",
      "82/82 [==============================] - 16s 189ms/step - loss: 0.2556 - accuracy: 0.9400 - val_loss: 0.4740 - val_accuracy: 0.8783 - lr: 1.0000e-04\n",
      "Epoch 6/50\n",
      "82/82 [==============================] - 16s 191ms/step - loss: 0.1984 - accuracy: 0.9469 - val_loss: 0.2777 - val_accuracy: 0.9122 - lr: 1.0000e-04\n",
      "Epoch 7/50\n",
      "82/82 [==============================] - 15s 188ms/step - loss: 0.2394 - accuracy: 0.9434 - val_loss: 0.5717 - val_accuracy: 0.8767 - lr: 1.0000e-04\n",
      "Epoch 8/50\n",
      "82/82 [==============================] - 16s 192ms/step - loss: 0.1014 - accuracy: 0.9719 - val_loss: 0.1778 - val_accuracy: 0.9538 - lr: 1.0000e-04\n",
      "Epoch 9/50\n",
      "82/82 [==============================] - 16s 190ms/step - loss: 0.0766 - accuracy: 0.9769 - val_loss: 0.1899 - val_accuracy: 0.9476 - lr: 1.0000e-04\n",
      "Epoch 10/50\n",
      "82/82 [==============================] - 15s 188ms/step - loss: 0.1252 - accuracy: 0.9677 - val_loss: 0.4056 - val_accuracy: 0.9106 - lr: 1.0000e-04\n",
      "Epoch 11/50\n",
      "82/82 [==============================] - 16s 192ms/step - loss: 0.0866 - accuracy: 0.9761 - val_loss: 0.2902 - val_accuracy: 0.9168 - lr: 1.0000e-04\n",
      "Epoch 12/50\n",
      "82/82 [==============================] - 16s 191ms/step - loss: 0.0442 - accuracy: 0.9869 - val_loss: 0.0355 - val_accuracy: 0.9846 - lr: 1.0000e-05\n",
      "Epoch 13/50\n",
      "82/82 [==============================] - 15s 188ms/step - loss: 0.0192 - accuracy: 0.9935 - val_loss: 0.0478 - val_accuracy: 0.9769 - lr: 1.0000e-05\n",
      "Epoch 14/50\n",
      "82/82 [==============================] - 16s 195ms/step - loss: 0.0204 - accuracy: 0.9935 - val_loss: 0.0436 - val_accuracy: 0.9815 - lr: 1.0000e-05\n",
      "Epoch 15/50\n",
      "82/82 [==============================] - 15s 185ms/step - loss: 0.0091 - accuracy: 0.9969 - val_loss: 0.0383 - val_accuracy: 0.9831 - lr: 1.0000e-05\n",
      "Epoch 16/50\n",
      "82/82 [==============================] - 15s 186ms/step - loss: 0.0183 - accuracy: 0.9942 - val_loss: 0.0385 - val_accuracy: 0.9846 - lr: 1.0000e-05\n",
      "Epoch 17/50\n",
      "82/82 [==============================] - 15s 185ms/step - loss: 0.0256 - accuracy: 0.9958 - val_loss: 0.0373 - val_accuracy: 0.9846 - lr: 1.0000e-05\n",
      "Epoch 18/50\n",
      "82/82 [==============================] - 15s 185ms/step - loss: 0.0188 - accuracy: 0.9950 - val_loss: 0.0364 - val_accuracy: 0.9861 - lr: 1.0000e-05\n",
      "Epoch 19/50\n",
      "82/82 [==============================] - 16s 191ms/step - loss: 0.0226 - accuracy: 0.9954 - val_loss: 0.0412 - val_accuracy: 0.9861 - lr: 1.0000e-05\n",
      "Epoch 20/50\n",
      "82/82 [==============================] - 15s 187ms/step - loss: 0.0032 - accuracy: 0.9992 - val_loss: 0.0356 - val_accuracy: 0.9846 - lr: 1.0000e-05\n",
      "Epoch 21/50\n",
      "82/82 [==============================] - 16s 191ms/step - loss: 0.0062 - accuracy: 0.9985 - val_loss: 0.0350 - val_accuracy: 0.9815 - lr: 1.0000e-05\n",
      "Epoch 22/50\n",
      "82/82 [==============================] - 15s 188ms/step - loss: 0.0103 - accuracy: 0.9977 - val_loss: 0.0361 - val_accuracy: 0.9815 - lr: 1.0000e-05\n",
      "Epoch 23/50\n",
      "82/82 [==============================] - 16s 190ms/step - loss: 0.0083 - accuracy: 0.9988 - val_loss: 0.0336 - val_accuracy: 0.9877 - lr: 1.0000e-05\n",
      "Epoch 24/50\n",
      "82/82 [==============================] - 16s 191ms/step - loss: 0.0166 - accuracy: 0.9954 - val_loss: 0.0305 - val_accuracy: 0.9877 - lr: 1.0000e-05\n",
      "Epoch 25/50\n",
      "82/82 [==============================] - 16s 189ms/step - loss: 0.0089 - accuracy: 0.9958 - val_loss: 0.0265 - val_accuracy: 0.9908 - lr: 1.0000e-05\n",
      "Epoch 26/50\n",
      "82/82 [==============================] - 15s 188ms/step - loss: 0.0243 - accuracy: 0.9962 - val_loss: 0.0371 - val_accuracy: 0.9877 - lr: 1.0000e-05\n",
      "Epoch 27/50\n",
      "82/82 [==============================] - 15s 188ms/step - loss: 0.0153 - accuracy: 0.9946 - val_loss: 0.0409 - val_accuracy: 0.9846 - lr: 1.0000e-05\n",
      "Epoch 28/50\n",
      "82/82 [==============================] - 15s 189ms/step - loss: 0.0047 - accuracy: 0.9988 - val_loss: 0.0221 - val_accuracy: 0.9877 - lr: 1.0000e-05\n",
      "Epoch 29/50\n",
      "82/82 [==============================] - 15s 188ms/step - loss: 0.0039 - accuracy: 0.9988 - val_loss: 0.0288 - val_accuracy: 0.9861 - lr: 1.0000e-05\n",
      "Epoch 30/50\n",
      "82/82 [==============================] - 16s 190ms/step - loss: 0.0029 - accuracy: 0.9988 - val_loss: 0.0205 - val_accuracy: 0.9923 - lr: 1.0000e-05\n",
      "Epoch 31/50\n",
      "82/82 [==============================] - 15s 188ms/step - loss: 0.0045 - accuracy: 0.9988 - val_loss: 0.0275 - val_accuracy: 0.9892 - lr: 1.0000e-05\n",
      "Epoch 32/50\n",
      "82/82 [==============================] - 15s 188ms/step - loss: 0.0048 - accuracy: 0.9981 - val_loss: 0.0255 - val_accuracy: 0.9877 - lr: 1.0000e-05\n",
      "Epoch 33/50\n",
      "82/82 [==============================] - 15s 188ms/step - loss: 0.0030 - accuracy: 0.9985 - val_loss: 0.0414 - val_accuracy: 0.9800 - lr: 1.0000e-05\n",
      "Epoch 34/50\n",
      "82/82 [==============================] - 15s 187ms/step - loss: 0.0023 - accuracy: 0.9992 - val_loss: 0.0421 - val_accuracy: 0.9800 - lr: 1.0000e-05\n",
      "Epoch 35/50\n",
      "82/82 [==============================] - 15s 187ms/step - loss: 0.0041 - accuracy: 0.9985 - val_loss: 0.0335 - val_accuracy: 0.9831 - lr: 1.0000e-05\n",
      "Epoch 36/50\n",
      "82/82 [==============================] - 16s 190ms/step - loss: 0.0040 - accuracy: 0.9992 - val_loss: 0.0340 - val_accuracy: 0.9861 - lr: 1.0000e-05\n",
      "Epoch 37/50\n",
      "82/82 [==============================] - 16s 192ms/step - loss: 0.0027 - accuracy: 0.9992 - val_loss: 0.0285 - val_accuracy: 0.9892 - lr: 1.0000e-05\n",
      "Epoch 38/50\n",
      "82/82 [==============================] - 15s 189ms/step - loss: 0.0020 - accuracy: 0.9988 - val_loss: 0.0311 - val_accuracy: 0.9908 - lr: 1.0000e-05\n",
      "Epoch 39/50\n",
      "82/82 [==============================] - 16s 191ms/step - loss: 0.0040 - accuracy: 0.9981 - val_loss: 0.0364 - val_accuracy: 0.9877 - lr: 1.0000e-05\n",
      "Epoch 40/50\n",
      "82/82 [==============================] - 16s 190ms/step - loss: 0.0026 - accuracy: 0.9992 - val_loss: 0.0265 - val_accuracy: 0.9861 - lr: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "def train_final_model(model_builder, epochs=20):\n",
    "    # Crear el modelo final utilizando el modelo_builder\n",
    "    final_model = model_builder()\n",
    "\n",
    "    # Concatenar los datos de entrenamiento y validación\n",
    "    X = np.concatenate([x for x, y in train_data])\n",
    "    y = np.concatenate([y for x, y in train_data])\n",
    "\n",
    "    # Definir el callback de EarlyStopping y ReduceLROnPlateau\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, min_lr=0.00001)\n",
    "\n",
    "    # Agregar los callbacks al método fit\n",
    "    callbacks = [early_stopping, reduce_lr]\n",
    "    final_model.fit(\n",
    "        X,\n",
    "        y, \n",
    "        epochs=epochs, \n",
    "        batch_size=batch_size, \n",
    "        validation_data=val_data, \n",
    "        callbacks=callbacks)\n",
    "\n",
    "    # Devolver el modelo final entrenado\n",
    "    return final_model\n",
    "\n",
    "# Entrenar el modelo final utilizando build_model y 50 epochs\n",
    "final_model = train_final_model(build_model_custom,50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 1s 43ms/step - loss: 0.0351 - accuracy: 0.9889\n",
      "Test Loss: 0.0351\n",
      "Test Accuracy: 0.9889\n",
      "26/26 [==============================] - 1s 42ms/step\n",
      "Classification Report:\n",
      "                                            precision    recall  f1-score   support\n",
      "\n",
      "                         Grape___Black_rot       0.99      0.97      0.98       236\n",
      "              Grape___Esca_(Black_Measles)       0.98      0.99      0.98       277\n",
      "Grape___Leaf_blight_(Isariopsis_Leaf_Spot)       1.00      1.00      1.00       216\n",
      "                           Grape___healthy       1.00      1.00      1.00        85\n",
      "\n",
      "                                  accuracy                           0.99       814\n",
      "                                 macro avg       0.99      0.99      0.99       814\n",
      "                              weighted avg       0.99      0.99      0.99       814\n",
      "\n",
      "Confusion Matrix:\n",
      "[[229   7   0   0]\n",
      " [  2 275   0   0]\n",
      " [  0   0 216   0]\n",
      " [  0   0   0  85]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo en el conjunto de prueba\n",
    "test_loss, test_accuracy = final_model.evaluate(test_data)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "# Predecir el conjunto de prueba\n",
    "Y_pred = final_model.predict(test_data)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "# Obtener las etiquetas del conjunto de prueba\n",
    "y_true = np.concatenate([y.numpy() for _, y in test_data.unbatch()])\n",
    "y_true_labels = np.argmax(y_true.reshape(-1, len(class_names)), axis=1)\n",
    "\n",
    "# Mostrar el informe de clasificación\n",
    "print('Classification Report:')\n",
    "print(classification_report(y_true_labels, y_pred, target_names=class_names))\n",
    "\n",
    "# Mostrar la matriz de confusión\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_true_labels, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
